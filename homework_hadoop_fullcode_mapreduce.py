# -*- coding: utf-8 -*-
"""Homework Hadoop Fullcode - MapReduce.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mmk5LNlQs8ToZOTqiJq7Bn-a3hksTMvF
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!java -version

!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java
!update-alternatives --set javac /usr/lib/jvm/java-8-openjdk-amd64/bin/javac
!update-alternatives --set jps /usr/lib/jvm/java-8-openjdk-amd64/bin/jps
!java -version

#Finding the default Java path
!readlink -f /usr/bin/java | sed "s:bin/java::"
!apt-get install openssh-server -qq > /dev/null
!service ssh start

!grep Port /etc/ssh/sshd_config

#Creating a new rsa key pair with empty password
!ssh-keygen -t rsa -P "" -f ~/.ssh/id_rsa <<< y

# See id_rsa.pub content
!more /root/.ssh/id_rsa.pub

#Copying the key to autorized keys
!cat $HOME/.ssh/id_rsa.pub > $HOME/.ssh/authorized_keys
#Changing the permissions on the key
!chmod 0600 ~/.ssh/authorized_keys

#Conneting with the local machine
!ssh -o StrictHostKeyChecking=no localhost uptime


#Downloading Hadoop 3.2.3
!wget -q https://archive.apache.org/dist/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz

#Untarring the file
!sudo tar -xzf hadoop-3.2.3.tar.gz
#Removing the tar file
!rm hadoop-3.2.3.tar.gz


#Copying the hadoop files to user/local
!cp -r hadoop-3.2.3/ /usr/local/
#-r copy directories recursively

#Adding JAVA_HOME directory to hadoop-env.sh file
!sed -i '/export JAVA_HOME=/a export JAVA_HOME=\/usr\/lib\/jvm\/java-8-openjdk-amd64' /usr/local/hadoop-3.2.3/etc/hadoop/hadoop-env.sh

import os
#Creating environment variables
#Creating Hadoop home variable

os.environ["HADOOP_HOME"] = "/usr/local/hadoop-3.2.3"
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["JRE_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64/jre"
os.environ["PATH"] += f'{os.environ["JAVA_HOME"]}/bin:{os.environ["JRE_HOME"]}/bin:{os.environ["HADOOP_HOME"]}/sbin'

# Commented out IPython magic to ensure Python compatibility.
# #Adding required property to core-site.xlm file
# %%bash
# cat <<EOF > $HADOOP_HOME/etc/hadoop/core-site.xml
# <?xml version="1.0" encoding="UTF-8"?>
# <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
# <!--
#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License. See accompanying LICENSE file.
# -->
# 
# <!-- Put site-specific property overrides in this file. -->
# 
# <configuration>
#   <property>
#           <name>fs.defaultFS</name>
#           <value>hdfs://localhost:9000</value>
#           <description>Where HDFS NameNode can be found on the network</description>
#   </property>
# </configuration>
# EOF

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# cat <<EOF > $HADOOP_HOME/etc/hadoop/hdfs-site.xml
# <?xml version="1.0" encoding="UTF-8"?>
# <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
# <!--
#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License. See accompanying LICENSE file.
# -->
# 
# <!-- Put site-specific property overrides in this file. -->
# 
# <configuration>
# <property>
#     <name>dfs.replication</name>
#     <value>1</value>
#   </property>
# 
# </configuration>
# EOF

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# cat <<EOF > $HADOOP_HOME/etc/hadoop/mapred-site.xml
# <?xml version="1.0"?>
# <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
# <!--
#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License. See accompanying LICENSE file.
# -->
# 
# <!-- Put site-specific property overrides in this file. -->
# 
# <configuration>
# <property>
#     <name>mapreduce.framework.name</name>
#     <value>yarn</value>
#   </property>
#   <property>
#     <name>mapreduce.application.classpath</name>
#     <value>$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*</value>
#   </property>
# 
# </configuration>
# EOF

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# cat <<EOF > $HADOOP_HOME/etc/hadoop/yarn-site.xml
# <?xml version="1.0"?>
# <!--
#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License. See accompanying LICENSE file.
# -->
# <configuration>
# <property>
#     <description>The hostname of the RM.</description>
#     <name>yarn.resourcemanager.hostname</name>
#     <value>localhost</value>
#   </property>
#   <property>
#     <name>yarn.nodemanager.aux-services</name>
#     <value>mapreduce_shuffle</value>
#   </property>
#   <property>
#     <name>yarn.nodemanager.env-whitelist</name>
#     <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME</value>
#   </property>
# 
# <!-- Site specific YARN configuration properties -->
# 
# </configuration>
# EOF

"""# Formatting the HDFS Filesystem

Before HDFS can be used for the first time the file system must be formatted. The formatting process creates an empty file system by creating the storage directories and the initial versions of the NameNodes
"""

!$HADOOP_HOME/bin/hdfs namenode -format

#Creating other necessary enviroment variables before starting nodes
os.environ["HDFS_NAMENODE_USER"] = "root"
os.environ["HDFS_DATANODE_USER"] = "root"
os.environ["HDFS_SECONDARYNAMENODE_USER"] = "root"
os.environ["YARN_RESOURCEMANAGER_USER"] = "root"
os.environ["YARN_NODEMANAGER_USER"] = "root"

#Launching hdfs deamons
!$HADOOP_HOME/sbin/start-dfs.sh

#Launching yarn deamons
#nohup causes a process to ignore a SIGHUP signal
!nohup $HADOOP_HOME/sbin/start-yarn.sh

#Listing the running deamons
!jps

"""### Monitoring Hadoop cluster with hadoop admin commands"""

#Report the basic file system information and statistics
!$HADOOP_HOME/bin/hdfs dfsadmin -report

"""# MapReduce"""

while True: pass

#Creating directory in HDFS
!$HADOOP_HOME/bin/hdfs dfs -mkdir /word_count
#Coping file from local file system to HDFS
!$HADOOP_HOME/bin/hdfs dfs -put pembukaan_uud1945.txt /word_count

#Exploring Hadoop folder
!$HADOOP_HOME/bin/hdfs dfs -ls /word_count

# Run MapReduce Example using JAVA
!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar wordcount /word_count/pembukaan_uud1945.txt /word_count/output/

#Exploring the created output directory
#part-r-00000 contains the actual ouput
!$HADOOP_HOME/bin/hdfs dfs -ls /word_count/output

#Printing
!$HADOOP_HOME/bin/hdfs dfs -cat /word_count/output/part-r-00000

"""# Hadoop Streaming Using Python

Hadoop Streaming is a feature that comes with Hadoop and allows users or developers to use various different languages for writing MapReduce programs like Python, C++, Ruby, etc.

The utility will create a Map/Reduce job, submit the job to an appropriate cluster, and monitor the progress of the job until it completes.
"""

#Exploring Hadoop utilities available
!ls $HADOOP_HOME/share/hadoop/tools/lib/

#Creating directory in HDFS
!$HADOOP_HOME/bin/hdfs dfs -mkdir /word_count_with_python

#Copying the file from local file system to Hadoop distributed file system (HDFS)
!$HADOOP_HOME/bin/hdfs dfs -put pembukaan_uud1945.txt /word_count_with_python

"""## Mapper

The mapper is an executable that reads all input records from a file/s and generates an output in the form of key-value pairs which works as input for the Reducer.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile mapper.py
# 
# #!/usr/bin/env python
# import sys
# 
# # Reading each line from input (stdin)
# for line in sys.stdin:
#     # Stripping any leading/trailing spaces
#     line = line.strip()
#     # Splitting the line into words
#     words = line.split()
#     # Output the word with a count of 1
#     for word in words:
#         print(f'{word}\t1')

"""## Reducer

The reducer is an executable that reads all the intermediate key-value pairs generated by the mapper and generates a final output as a result of a computation operation like addition, filtration, and aggregation.

Both the mapper and the reducer read the input from stdin (line by line) and emit the output to stdout.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile reducer.py
# 
# #!/usr/bin/env python
# import sys
# 
# current_word = None
# current_count = 0
# word = None
# 
# # Reading each line from the input (stdin)
# for line in sys.stdin:
#     # Stripping leading and trailing spaces
#     line = line.strip()
#     # Splitting the line into word and count
#     word, count = line.split('\t', 1)
# 
#     # Convert count from string to int
#     try:
#         count = int(count)
#     except ValueError:
#         continue
# 
#     # If the word is the same as the previous one, increment its count
#     if current_word == word:
#         current_count += count
#     else:
#         if current_word:
#             # Output the word and its count
#             print(f'{current_word} {current_count}')
#         current_count = count
#         current_word = word
# 
# # Output the last word if any
# if current_word == word:
#     print(f'{current_word} {current_count}')

#Testing our MapReduce job locally (Hadoop does not participate here)
!cat pembukaan_uud1945.txt | python mapper.py | sort -k1,1 | python reducer.py | head -10
#We apply sorting after the mapper because it is the default operation in MapReduce architecture

#Changing the permissions of the files
!chmod 777 /content/mapper.py /content/reducer.py
#Setting 777 permissions to a file or directory means that it will be readable, writable and executable by all users

#Running MapReduce programs
!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar \
  -input /word_count_with_python/pembukaan_uud1945.txt \
  -output /word_count_with_python/output \
  -mapper "python /content/mapper.py" \
  -reducer "python /content/reducer.py"

#Exploring the created output directory
#part-r-00000 contains the actual ouput
!$HADOOP_HOME/bin/hdfs dfs -ls /word_count_with_python/output

#Printing
!$HADOOP_HOME/bin/hdfs dfs -cat /word_count_with_python/output/part-00000

!$HADOOP_HOME/bin/hdfs dfs -copyToLocal /word_count_with_python/output/part-00000 /content/output_word_count.txt

# Install git
!apt-get install git -y

# Set up git
!git config --global user.name "farhanseptiannugraha"
!git config --global user.email "farhan.septian101112@gmail.com"

# Initialize git
!git init

# Add files to staging
!git add mapper.py reducer.py output_word_count.txt homework_hadoop_fullcode_mapreduce.py

# Commit files
!git commit -m "Homework Hadoop"

# Add remote repository (Replace with your correct GitHub URL)
!git remote add origin https://github.com/farhanseptiannugraha/homework_hadoop.git

# Pull latest changes from GitHub to avoid conflicts
!git pull origin main --rebase

# Push to GitHub
!git branch -M main
!git push -u origin main